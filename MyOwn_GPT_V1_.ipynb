{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Device Setting"
      ],
      "metadata": {
        "id": "DpGlsjhSF0NA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# Check if CUDA (GPU support) is available\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "# If CUDA is available, this should print the name of the GPU (e.g., Tesla K80, T4, P100, etc.)\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU available')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfH1vrXK6nLl",
        "outputId": "298b99dc-c249-48ce-a1db-8686491a8b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "nCOT-oDjF9KZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 64\n",
        "batch_size = 128\n",
        "max_iters = 3000\n",
        "eval_iters = 100\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "dropout = 0.2 # this will take out 20% of our neurons at random while training.\n",
        "n_embd = 384\n",
        "n_layer = 8 # NO. of Decoder Blocks\n",
        "n_head = 8\n"
      ],
      "metadata": {
        "id": "ji4S_uWmF6Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader"
      ],
      "metadata": {
        "id": "uHWie3c5GNfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = \"\"\n",
        "with open(\"wizard_of_oz.txt\", 'r', encoding= 'utf-8') as f:\n",
        "  text = f.read()\n",
        "  chars = sorted(list(set(text)))\n",
        "\n",
        "vocab_size = len(chars)"
      ],
      "metadata": {
        "id": "0s-yChu2GPvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder and Decoder"
      ],
      "metadata": {
        "id": "Z11SfPPrGa8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_string = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "wvxoicWBGdWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Validation Splits"
      ],
      "metadata": {
        "id": "2_edYoAxG92O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.8*(len(data)))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "I1az0wQxGw2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function randomly selects batches of data from either the training or validation dataset, given a specified split, and creates input-output pairs for a sequence modeling task with a sliding window approach, where each input sequence (x) is of length block_size and the corresponding output sequence (y) is shifted by one token."
      ],
      "metadata": {
        "id": "pu76_WzMHArM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "  x = torch.stack([data[i: i + block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1 : i+block_size+1] for i in ix])\n",
        "  x,y = x.to(device), y.to(device)\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "z6oMC-l3HB01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Going deeper into the Block Module(each Decoder Layer)"
      ],
      "metadata": {
        "id": "Ai5-HX-WVQF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    # n_embd: embedding dimension, n_head: no. of heads we'd like:))\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head # How many features are each of the heads capturing\n",
        "    self.sa = MultiHeadAttention(n_head, head_size) #sa: self-attention\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd) # Layer Norm essentially helps in smoothing features out\n",
        "    self.ln2 = nn.LaterNorm(n_embd)\n",
        "\n",
        "  def forward(self,x):\n",
        "    y = self.sa(x)\n",
        "    x = self.ln1(x + y)\n",
        "    y = self.ffwd(x)\n",
        "    x = self.ln2(x + y)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "zrHmOpUkVaTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  # A simple linear layer followed by a non-linear layer, and again a linear one.\n",
        "  # The inner dimensions of both the linear functions should match because they will be multiplied. So the final output weill be of (n_embd,n_embd) in size.\n",
        "  def __init__(self,n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4* n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4* n_embd, n_embd),\n",
        "        nn.Dropout(dropout), # Certain % of our neurons stop learning. Stops overfitting:)) By becoming 0???\n",
        "    )\n",
        "\n",
        "    def forward(self,x):\n",
        "      return self.net(x)\n"
      ],
      "metadata": {
        "id": "q2NPTtQSYzRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  # Multiple heads of self-attention in parallel.\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    # Initializing a linear projection layer to combine the outputs of all heads into a single output with size 'n_embd'. Helps add the bias parameter that's learnable. The more the parameters we can train, the better. - Check!?\n",
        "    self.proj = nn.Linear(head_size * num_heads, n_embd) # This will project head_size * num_heads to the n_embd. Is a common practice and helps in regularization - wtf???ALSO!!!!Like the one below!\n",
        "    self.dropout = nn.Dropout(dropout) # Applying regularization - ???\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim= -1) # (B,T,C) -->  (B, T, head_size * num_heads).\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "ifJb4OHcbGBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "\n",
        "The \"__init__\" function:\n",
        "\n",
        "- It initializes linear layers (key, query, and value) to transform the input tensor x into queries, keys, and values, respectively.\n",
        "\n",
        "- It initializes a buffer tril containing a lower triangular matrix, which will be used to mask attention scores to prevent attending to future tokens.\n",
        "\n",
        "The \"__forward__\" function:\n",
        "- The parameter n_embd is the dimensionality of the embedding vectors. However, in this context, it represents the number of features in the input tensor x.\n",
        "\n",
        "  The purpose of the linear transformations is to project the input features into different subspaces, allowing the model to learn different aspects of the input for keys, queries, and values.\n",
        "\n",
        "- ![image.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "Afe4OUxQYkd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\" One head of Self-Attention Only!!! \"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias = False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias = False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # we register our model state instead of initializing all the above every single time every single layer\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # input size: (batch, time-step, channels/classes/features)\n",
        "    # output size: (batch, time-step, head-size)\n",
        "\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x) # (B,T,head_size)\n",
        "    q = self.query(x)\n",
        "    # compute attention scores (\"affinities\")\n",
        "    wei = q @ k.transpose(-2,-1) * k.shape[-1]** -0.5 # (B,T,head_size) @ (B,head_size, T) --> (B,T,T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T) # To prevent look-ahead:))\n",
        "    wei = F.softmax(wei, dim = -1) # (B,T,T) # Sharpening our features we got from our attenion affinities. Making our model more confident about greater attention affinities!!!\n",
        "    # perform the weighted aggregation of the values\n",
        "    v = self.value(x) # (B,T, head_size)\n",
        "    out = wei @ v # (B,T,T) @ (B,T,head_size) --> (B,T, head_size)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kjtBKbj5gsaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard Deviation for Model Parameters\n",
        "\n",
        "If the numbers are far from eachother, the standard deviation will be very high. SO if you have outliers, then the network is going to face trouble in the learning process.\n",
        "\n",
        "But if all the numbers are very close to each other and very similar, then all the neurons will learn the same patterns, and there would be no learning done."
      ],
      "metadata": {
        "id": "Yh0vK6vqTlhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.sa(x)\n",
        "        x = self.ln1(x + y)\n",
        "        y = self.ffwd(x)\n",
        "        x = self.ln2(x + y)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, index, targets=None):\n",
        "        B, T = index.shape\n",
        "\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, index, max_new_tokens):\n",
        "        # index is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            index_cond = index[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self.forward(index_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
        "        return index\n",
        "\n",
        "model = GPTLanguageModel(vocab_size)\n",
        "# print('loading model parameters...')\n",
        "# with open('model-01.pkl', 'rb') as f:\n",
        "#     model = pickle.load(f)\n",
        "# print('loaded successfully!')\n",
        "#device = torch.device('cpu')\n",
        "m = model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "N636hrKNpeH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to Estimate the Loss"
      ],
      "metadata": {
        "id": "iq0y3zpNHVka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "B5tDOeZ_qEK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    print(iter)\n",
        "    if iter % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model.forward(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awiOr2Gfrv8M",
        "outputId": "fdfe4fa7-b541-4164-c4f8-c853875dbbe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "step: 0, train loss: 4.591, val loss: 4.591\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "step: 100, train loss: 3.147, val loss: 3.260\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "step: 200, train loss: 3.155, val loss: 3.279\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "step: 300, train loss: 3.148, val loss: 3.245\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "step: 400, train loss: 3.144, val loss: 3.251\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "step: 500, train loss: 3.146, val loss: 3.250\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "step: 600, train loss: 3.148, val loss: 3.252\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "step: 700, train loss: 3.145, val loss: 3.245\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "step: 800, train loss: 3.144, val loss: 3.256\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "step: 900, train loss: 3.145, val loss: 3.256\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "step: 1000, train loss: 3.142, val loss: 3.247\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "step: 1100, train loss: 3.148, val loss: 3.258\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n",
            "1125\n",
            "1126\n",
            "1127\n",
            "1128\n",
            "1129\n",
            "1130\n",
            "1131\n",
            "1132\n",
            "1133\n",
            "1134\n",
            "1135\n",
            "1136\n",
            "1137\n",
            "1138\n",
            "1139\n",
            "1140\n",
            "1141\n",
            "1142\n",
            "1143\n",
            "1144\n",
            "1145\n",
            "1146\n",
            "1147\n",
            "1148\n",
            "1149\n",
            "1150\n",
            "1151\n",
            "1152\n",
            "1153\n",
            "1154\n",
            "1155\n",
            "1156\n",
            "1157\n",
            "1158\n",
            "1159\n",
            "1160\n",
            "1161\n",
            "1162\n",
            "1163\n",
            "1164\n",
            "1165\n",
            "1166\n",
            "1167\n",
            "1168\n",
            "1169\n",
            "1170\n",
            "1171\n",
            "1172\n",
            "1173\n",
            "1174\n",
            "1175\n",
            "1176\n",
            "1177\n",
            "1178\n",
            "1179\n",
            "1180\n",
            "1181\n",
            "1182\n",
            "1183\n",
            "1184\n",
            "1185\n",
            "1186\n",
            "1187\n",
            "1188\n",
            "1189\n",
            "1190\n",
            "1191\n",
            "1192\n",
            "1193\n",
            "1194\n",
            "1195\n",
            "1196\n",
            "1197\n",
            "1198\n",
            "1199\n",
            "1200\n",
            "step: 1200, train loss: 3.149, val loss: 3.256\n",
            "1201\n",
            "1202\n",
            "1203\n",
            "1204\n",
            "1205\n",
            "1206\n",
            "1207\n",
            "1208\n",
            "1209\n",
            "1210\n",
            "1211\n",
            "1212\n",
            "1213\n",
            "1214\n",
            "1215\n",
            "1216\n",
            "1217\n",
            "1218\n",
            "1219\n",
            "1220\n",
            "1221\n",
            "1222\n",
            "1223\n",
            "1224\n",
            "1225\n",
            "1226\n",
            "1227\n",
            "1228\n",
            "1229\n",
            "1230\n",
            "1231\n",
            "1232\n",
            "1233\n",
            "1234\n",
            "1235\n",
            "1236\n",
            "1237\n",
            "1238\n",
            "1239\n",
            "1240\n",
            "1241\n",
            "1242\n",
            "1243\n",
            "1244\n",
            "1245\n",
            "1246\n",
            "1247\n",
            "1248\n",
            "1249\n",
            "1250\n",
            "1251\n",
            "1252\n",
            "1253\n",
            "1254\n",
            "1255\n",
            "1256\n",
            "1257\n",
            "1258\n",
            "1259\n",
            "1260\n",
            "1261\n",
            "1262\n",
            "1263\n",
            "1264\n",
            "1265\n",
            "1266\n",
            "1267\n",
            "1268\n",
            "1269\n",
            "1270\n",
            "1271\n",
            "1272\n",
            "1273\n",
            "1274\n",
            "1275\n",
            "1276\n",
            "1277\n",
            "1278\n",
            "1279\n",
            "1280\n",
            "1281\n",
            "1282\n",
            "1283\n",
            "1284\n",
            "1285\n",
            "1286\n",
            "1287\n",
            "1288\n",
            "1289\n",
            "1290\n",
            "1291\n",
            "1292\n",
            "1293\n",
            "1294\n",
            "1295\n",
            "1296\n",
            "1297\n",
            "1298\n",
            "1299\n",
            "1300\n",
            "step: 1300, train loss: 3.152, val loss: 3.263\n",
            "1301\n",
            "1302\n",
            "1303\n",
            "1304\n",
            "1305\n",
            "1306\n",
            "1307\n",
            "1308\n",
            "1309\n",
            "1310\n",
            "1311\n",
            "1312\n",
            "1313\n",
            "1314\n",
            "1315\n",
            "1316\n",
            "1317\n",
            "1318\n",
            "1319\n",
            "1320\n",
            "1321\n",
            "1322\n",
            "1323\n",
            "1324\n",
            "1325\n",
            "1326\n",
            "1327\n",
            "1328\n",
            "1329\n",
            "1330\n",
            "1331\n",
            "1332\n",
            "1333\n",
            "1334\n",
            "1335\n",
            "1336\n",
            "1337\n",
            "1338\n",
            "1339\n",
            "1340\n",
            "1341\n",
            "1342\n",
            "1343\n",
            "1344\n",
            "1345\n",
            "1346\n",
            "1347\n",
            "1348\n",
            "1349\n",
            "1350\n",
            "1351\n",
            "1352\n",
            "1353\n",
            "1354\n",
            "1355\n",
            "1356\n",
            "1357\n",
            "1358\n",
            "1359\n",
            "1360\n",
            "1361\n",
            "1362\n",
            "1363\n",
            "1364\n",
            "1365\n",
            "1366\n",
            "1367\n",
            "1368\n",
            "1369\n",
            "1370\n",
            "1371\n",
            "1372\n",
            "1373\n",
            "1374\n",
            "1375\n",
            "1376\n",
            "1377\n",
            "1378\n",
            "1379\n",
            "1380\n",
            "1381\n",
            "1382\n",
            "1383\n",
            "1384\n",
            "1385\n",
            "1386\n",
            "1387\n",
            "1388\n",
            "1389\n",
            "1390\n",
            "1391\n",
            "1392\n",
            "1393\n",
            "1394\n",
            "1395\n",
            "1396\n",
            "1397\n",
            "1398\n",
            "1399\n",
            "1400\n",
            "step: 1400, train loss: 3.146, val loss: 3.257\n",
            "1401\n",
            "1402\n",
            "1403\n",
            "1404\n",
            "1405\n",
            "1406\n",
            "1407\n",
            "1408\n",
            "1409\n",
            "1410\n",
            "1411\n",
            "1412\n",
            "1413\n",
            "1414\n",
            "1415\n",
            "1416\n",
            "1417\n",
            "1418\n",
            "1419\n",
            "1420\n",
            "1421\n",
            "1422\n",
            "1423\n",
            "1424\n",
            "1425\n",
            "1426\n",
            "1427\n",
            "1428\n",
            "1429\n",
            "1430\n",
            "1431\n",
            "1432\n",
            "1433\n",
            "1434\n",
            "1435\n",
            "1436\n",
            "1437\n",
            "1438\n",
            "1439\n",
            "1440\n",
            "1441\n",
            "1442\n",
            "1443\n",
            "1444\n",
            "1445\n",
            "1446\n",
            "1447\n",
            "1448\n",
            "1449\n",
            "1450\n",
            "1451\n",
            "1452\n",
            "1453\n",
            "1454\n",
            "1455\n",
            "1456\n",
            "1457\n",
            "1458\n",
            "1459\n",
            "1460\n",
            "1461\n",
            "1462\n",
            "1463\n",
            "1464\n",
            "1465\n",
            "1466\n",
            "1467\n",
            "1468\n",
            "1469\n",
            "1470\n",
            "1471\n",
            "1472\n",
            "1473\n",
            "1474\n",
            "1475\n",
            "1476\n",
            "1477\n",
            "1478\n",
            "1479\n",
            "1480\n",
            "1481\n",
            "1482\n",
            "1483\n",
            "1484\n",
            "1485\n",
            "1486\n",
            "1487\n",
            "1488\n",
            "1489\n",
            "1490\n",
            "1491\n",
            "1492\n",
            "1493\n",
            "1494\n",
            "1495\n",
            "1496\n",
            "1497\n",
            "1498\n",
            "1499\n",
            "1500\n",
            "step: 1500, train loss: 3.148, val loss: 3.250\n",
            "1501\n",
            "1502\n",
            "1503\n",
            "1504\n",
            "1505\n",
            "1506\n",
            "1507\n",
            "1508\n",
            "1509\n",
            "1510\n",
            "1511\n",
            "1512\n",
            "1513\n",
            "1514\n",
            "1515\n",
            "1516\n",
            "1517\n",
            "1518\n",
            "1519\n",
            "1520\n",
            "1521\n",
            "1522\n",
            "1523\n",
            "1524\n",
            "1525\n",
            "1526\n",
            "1527\n",
            "1528\n",
            "1529\n",
            "1530\n",
            "1531\n",
            "1532\n",
            "1533\n",
            "1534\n",
            "1535\n",
            "1536\n",
            "1537\n",
            "1538\n",
            "1539\n",
            "1540\n",
            "1541\n",
            "1542\n",
            "1543\n",
            "1544\n",
            "1545\n",
            "1546\n",
            "1547\n",
            "1548\n",
            "1549\n",
            "1550\n",
            "1551\n",
            "1552\n",
            "1553\n",
            "1554\n",
            "1555\n",
            "1556\n",
            "1557\n",
            "1558\n",
            "1559\n",
            "1560\n",
            "1561\n",
            "1562\n",
            "1563\n",
            "1564\n",
            "1565\n",
            "1566\n",
            "1567\n",
            "1568\n",
            "1569\n",
            "1570\n",
            "1571\n",
            "1572\n",
            "1573\n",
            "1574\n",
            "1575\n",
            "1576\n",
            "1577\n",
            "1578\n",
            "1579\n",
            "1580\n",
            "1581\n",
            "1582\n",
            "1583\n",
            "1584\n",
            "1585\n",
            "1586\n",
            "1587\n",
            "1588\n",
            "1589\n",
            "1590\n",
            "1591\n",
            "1592\n",
            "1593\n",
            "1594\n",
            "1595\n",
            "1596\n",
            "1597\n",
            "1598\n",
            "1599\n",
            "1600\n",
            "step: 1600, train loss: 3.140, val loss: 3.246\n",
            "1601\n",
            "1602\n",
            "1603\n",
            "1604\n",
            "1605\n",
            "1606\n",
            "1607\n",
            "1608\n",
            "1609\n",
            "1610\n",
            "1611\n",
            "1612\n",
            "1613\n",
            "1614\n",
            "1615\n",
            "1616\n",
            "1617\n",
            "1618\n",
            "1619\n",
            "1620\n",
            "1621\n",
            "1622\n",
            "1623\n",
            "1624\n",
            "1625\n",
            "1626\n",
            "1627\n",
            "1628\n",
            "1629\n",
            "1630\n",
            "1631\n",
            "1632\n",
            "1633\n",
            "1634\n",
            "1635\n",
            "1636\n",
            "1637\n",
            "1638\n",
            "1639\n",
            "1640\n",
            "1641\n",
            "1642\n",
            "1643\n",
            "1644\n",
            "1645\n",
            "1646\n",
            "1647\n",
            "1648\n",
            "1649\n",
            "1650\n",
            "1651\n",
            "1652\n",
            "1653\n",
            "1654\n",
            "1655\n",
            "1656\n",
            "1657\n",
            "1658\n",
            "1659\n",
            "1660\n",
            "1661\n",
            "1662\n",
            "1663\n",
            "1664\n",
            "1665\n",
            "1666\n",
            "1667\n",
            "1668\n",
            "1669\n",
            "1670\n",
            "1671\n",
            "1672\n",
            "1673\n",
            "1674\n",
            "1675\n",
            "1676\n",
            "1677\n",
            "1678\n",
            "1679\n",
            "1680\n",
            "1681\n",
            "1682\n",
            "1683\n",
            "1684\n",
            "1685\n",
            "1686\n",
            "1687\n",
            "1688\n",
            "1689\n",
            "1690\n",
            "1691\n",
            "1692\n",
            "1693\n",
            "1694\n",
            "1695\n",
            "1696\n",
            "1697\n",
            "1698\n",
            "1699\n",
            "1700\n",
            "step: 1700, train loss: 3.151, val loss: 3.253\n",
            "1701\n",
            "1702\n",
            "1703\n",
            "1704\n",
            "1705\n",
            "1706\n",
            "1707\n",
            "1708\n",
            "1709\n",
            "1710\n",
            "1711\n",
            "1712\n",
            "1713\n",
            "1714\n",
            "1715\n",
            "1716\n",
            "1717\n",
            "1718\n",
            "1719\n",
            "1720\n",
            "1721\n",
            "1722\n",
            "1723\n",
            "1724\n",
            "1725\n",
            "1726\n",
            "1727\n",
            "1728\n",
            "1729\n",
            "1730\n",
            "1731\n",
            "1732\n",
            "1733\n",
            "1734\n",
            "1735\n",
            "1736\n",
            "1737\n",
            "1738\n",
            "1739\n",
            "1740\n",
            "1741\n",
            "1742\n",
            "1743\n",
            "1744\n",
            "1745\n",
            "1746\n",
            "1747\n",
            "1748\n",
            "1749\n",
            "1750\n",
            "1751\n",
            "1752\n",
            "1753\n",
            "1754\n",
            "1755\n",
            "1756\n",
            "1757\n",
            "1758\n",
            "1759\n",
            "1760\n",
            "1761\n",
            "1762\n",
            "1763\n",
            "1764\n",
            "1765\n",
            "1766\n",
            "1767\n",
            "1768\n",
            "1769\n",
            "1770\n",
            "1771\n",
            "1772\n",
            "1773\n",
            "1774\n",
            "1775\n",
            "1776\n",
            "1777\n",
            "1778\n",
            "1779\n",
            "1780\n",
            "1781\n",
            "1782\n",
            "1783\n",
            "1784\n",
            "1785\n",
            "1786\n",
            "1787\n",
            "1788\n",
            "1789\n",
            "1790\n",
            "1791\n",
            "1792\n",
            "1793\n",
            "1794\n",
            "1795\n",
            "1796\n",
            "1797\n",
            "1798\n",
            "1799\n",
            "1800\n",
            "step: 1800, train loss: 3.139, val loss: 3.266\n",
            "1801\n",
            "1802\n",
            "1803\n",
            "1804\n",
            "1805\n",
            "1806\n",
            "1807\n",
            "1808\n",
            "1809\n",
            "1810\n",
            "1811\n",
            "1812\n",
            "1813\n",
            "1814\n",
            "1815\n",
            "1816\n",
            "1817\n",
            "1818\n",
            "1819\n",
            "1820\n",
            "1821\n",
            "1822\n",
            "1823\n",
            "1824\n",
            "1825\n",
            "1826\n",
            "1827\n",
            "1828\n",
            "1829\n",
            "1830\n",
            "1831\n",
            "1832\n",
            "1833\n",
            "1834\n",
            "1835\n",
            "1836\n",
            "1837\n",
            "1838\n",
            "1839\n",
            "1840\n",
            "1841\n",
            "1842\n",
            "1843\n",
            "1844\n",
            "1845\n",
            "1846\n",
            "1847\n",
            "1848\n",
            "1849\n",
            "1850\n",
            "1851\n",
            "1852\n",
            "1853\n",
            "1854\n",
            "1855\n",
            "1856\n",
            "1857\n",
            "1858\n",
            "1859\n",
            "1860\n",
            "1861\n",
            "1862\n",
            "1863\n",
            "1864\n",
            "1865\n",
            "1866\n",
            "1867\n",
            "1868\n",
            "1869\n",
            "1870\n",
            "1871\n",
            "1872\n",
            "1873\n",
            "1874\n",
            "1875\n",
            "1876\n",
            "1877\n",
            "1878\n",
            "1879\n",
            "1880\n",
            "1881\n",
            "1882\n",
            "1883\n",
            "1884\n",
            "1885\n",
            "1886\n",
            "1887\n",
            "1888\n",
            "1889\n",
            "1890\n",
            "1891\n",
            "1892\n",
            "1893\n",
            "1894\n",
            "1895\n",
            "1896\n",
            "1897\n",
            "1898\n",
            "1899\n",
            "1900\n",
            "step: 1900, train loss: 3.141, val loss: 3.257\n",
            "1901\n",
            "1902\n",
            "1903\n",
            "1904\n",
            "1905\n",
            "1906\n",
            "1907\n",
            "1908\n",
            "1909\n",
            "1910\n",
            "1911\n",
            "1912\n",
            "1913\n",
            "1914\n",
            "1915\n",
            "1916\n",
            "1917\n",
            "1918\n",
            "1919\n",
            "1920\n",
            "1921\n",
            "1922\n",
            "1923\n",
            "1924\n",
            "1925\n",
            "1926\n",
            "1927\n",
            "1928\n",
            "1929\n",
            "1930\n",
            "1931\n",
            "1932\n",
            "1933\n",
            "1934\n",
            "1935\n",
            "1936\n",
            "1937\n",
            "1938\n",
            "1939\n",
            "1940\n",
            "1941\n",
            "1942\n",
            "1943\n",
            "1944\n",
            "1945\n",
            "1946\n",
            "1947\n",
            "1948\n",
            "1949\n",
            "1950\n",
            "1951\n",
            "1952\n",
            "1953\n",
            "1954\n",
            "1955\n",
            "1956\n",
            "1957\n",
            "1958\n",
            "1959\n",
            "1960\n",
            "1961\n",
            "1962\n",
            "1963\n",
            "1964\n",
            "1965\n",
            "1966\n",
            "1967\n",
            "1968\n",
            "1969\n",
            "1970\n",
            "1971\n",
            "1972\n",
            "1973\n",
            "1974\n",
            "1975\n",
            "1976\n",
            "1977\n",
            "1978\n",
            "1979\n",
            "1980\n",
            "1981\n",
            "1982\n",
            "1983\n",
            "1984\n",
            "1985\n",
            "1986\n",
            "1987\n",
            "1988\n",
            "1989\n",
            "1990\n",
            "1991\n",
            "1992\n",
            "1993\n",
            "1994\n",
            "1995\n",
            "1996\n",
            "1997\n",
            "1998\n",
            "1999\n",
            "2000\n",
            "step: 2000, train loss: 3.144, val loss: 3.253\n",
            "2001\n",
            "2002\n",
            "2003\n",
            "2004\n",
            "2005\n",
            "2006\n",
            "2007\n",
            "2008\n",
            "2009\n",
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n",
            "2019\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n",
            "2025\n",
            "2026\n",
            "2027\n",
            "2028\n",
            "2029\n",
            "2030\n",
            "2031\n",
            "2032\n",
            "2033\n",
            "2034\n",
            "2035\n",
            "2036\n",
            "2037\n",
            "2038\n",
            "2039\n",
            "2040\n",
            "2041\n",
            "2042\n",
            "2043\n",
            "2044\n",
            "2045\n",
            "2046\n",
            "2047\n",
            "2048\n",
            "2049\n",
            "2050\n",
            "2051\n",
            "2052\n",
            "2053\n",
            "2054\n",
            "2055\n",
            "2056\n",
            "2057\n",
            "2058\n",
            "2059\n",
            "2060\n",
            "2061\n",
            "2062\n",
            "2063\n",
            "2064\n",
            "2065\n",
            "2066\n",
            "2067\n",
            "2068\n",
            "2069\n",
            "2070\n",
            "2071\n",
            "2072\n",
            "2073\n",
            "2074\n",
            "2075\n",
            "2076\n",
            "2077\n",
            "2078\n",
            "2079\n",
            "2080\n",
            "2081\n",
            "2082\n",
            "2083\n",
            "2084\n",
            "2085\n",
            "2086\n",
            "2087\n",
            "2088\n",
            "2089\n",
            "2090\n",
            "2091\n",
            "2092\n",
            "2093\n",
            "2094\n",
            "2095\n",
            "2096\n",
            "2097\n",
            "2098\n",
            "2099\n",
            "2100\n",
            "step: 2100, train loss: 3.145, val loss: 3.259\n",
            "2101\n",
            "2102\n",
            "2103\n",
            "2104\n",
            "2105\n",
            "2106\n",
            "2107\n",
            "2108\n",
            "2109\n",
            "2110\n",
            "2111\n",
            "2112\n",
            "2113\n",
            "2114\n",
            "2115\n",
            "2116\n",
            "2117\n",
            "2118\n",
            "2119\n",
            "2120\n",
            "2121\n",
            "2122\n",
            "2123\n",
            "2124\n",
            "2125\n",
            "2126\n",
            "2127\n",
            "2128\n",
            "2129\n",
            "2130\n",
            "2131\n",
            "2132\n",
            "2133\n",
            "2134\n",
            "2135\n",
            "2136\n",
            "2137\n",
            "2138\n",
            "2139\n",
            "2140\n",
            "2141\n",
            "2142\n",
            "2143\n",
            "2144\n",
            "2145\n",
            "2146\n",
            "2147\n",
            "2148\n",
            "2149\n",
            "2150\n",
            "2151\n",
            "2152\n",
            "2153\n",
            "2154\n",
            "2155\n",
            "2156\n",
            "2157\n",
            "2158\n",
            "2159\n",
            "2160\n",
            "2161\n",
            "2162\n",
            "2163\n",
            "2164\n",
            "2165\n",
            "2166\n",
            "2167\n",
            "2168\n",
            "2169\n",
            "2170\n",
            "2171\n",
            "2172\n",
            "2173\n",
            "2174\n",
            "2175\n",
            "2176\n",
            "2177\n",
            "2178\n",
            "2179\n",
            "2180\n",
            "2181\n",
            "2182\n",
            "2183\n",
            "2184\n",
            "2185\n",
            "2186\n",
            "2187\n",
            "2188\n",
            "2189\n",
            "2190\n",
            "2191\n",
            "2192\n",
            "2193\n",
            "2194\n",
            "2195\n",
            "2196\n",
            "2197\n",
            "2198\n",
            "2199\n",
            "2200\n",
            "step: 2200, train loss: 3.145, val loss: 3.256\n",
            "2201\n",
            "2202\n",
            "2203\n",
            "2204\n",
            "2205\n",
            "2206\n",
            "2207\n",
            "2208\n",
            "2209\n",
            "2210\n",
            "2211\n",
            "2212\n",
            "2213\n",
            "2214\n",
            "2215\n",
            "2216\n",
            "2217\n",
            "2218\n",
            "2219\n",
            "2220\n",
            "2221\n",
            "2222\n",
            "2223\n",
            "2224\n",
            "2225\n",
            "2226\n",
            "2227\n",
            "2228\n",
            "2229\n",
            "2230\n",
            "2231\n",
            "2232\n",
            "2233\n",
            "2234\n",
            "2235\n",
            "2236\n",
            "2237\n",
            "2238\n",
            "2239\n",
            "2240\n",
            "2241\n",
            "2242\n",
            "2243\n",
            "2244\n",
            "2245\n",
            "2246\n",
            "2247\n",
            "2248\n",
            "2249\n",
            "2250\n",
            "2251\n",
            "2252\n",
            "2253\n",
            "2254\n",
            "2255\n",
            "2256\n",
            "2257\n",
            "2258\n",
            "2259\n",
            "2260\n",
            "2261\n",
            "2262\n",
            "2263\n",
            "2264\n",
            "2265\n",
            "2266\n",
            "2267\n",
            "2268\n",
            "2269\n",
            "2270\n",
            "2271\n",
            "2272\n",
            "2273\n",
            "2274\n",
            "2275\n",
            "2276\n",
            "2277\n",
            "2278\n",
            "2279\n",
            "2280\n",
            "2281\n",
            "2282\n",
            "2283\n",
            "2284\n",
            "2285\n",
            "2286\n",
            "2287\n",
            "2288\n",
            "2289\n",
            "2290\n",
            "2291\n",
            "2292\n",
            "2293\n",
            "2294\n",
            "2295\n",
            "2296\n",
            "2297\n",
            "2298\n",
            "2299\n",
            "2300\n",
            "step: 2300, train loss: 3.142, val loss: 3.256\n",
            "2301\n",
            "2302\n",
            "2303\n",
            "2304\n",
            "2305\n",
            "2306\n",
            "2307\n",
            "2308\n",
            "2309\n",
            "2310\n",
            "2311\n",
            "2312\n",
            "2313\n",
            "2314\n",
            "2315\n",
            "2316\n",
            "2317\n",
            "2318\n",
            "2319\n",
            "2320\n",
            "2321\n",
            "2322\n",
            "2323\n",
            "2324\n",
            "2325\n",
            "2326\n",
            "2327\n",
            "2328\n",
            "2329\n",
            "2330\n",
            "2331\n",
            "2332\n",
            "2333\n",
            "2334\n",
            "2335\n",
            "2336\n",
            "2337\n",
            "2338\n",
            "2339\n",
            "2340\n",
            "2341\n",
            "2342\n",
            "2343\n",
            "2344\n",
            "2345\n",
            "2346\n",
            "2347\n",
            "2348\n",
            "2349\n",
            "2350\n",
            "2351\n",
            "2352\n",
            "2353\n",
            "2354\n",
            "2355\n",
            "2356\n",
            "2357\n",
            "2358\n",
            "2359\n",
            "2360\n",
            "2361\n",
            "2362\n",
            "2363\n",
            "2364\n",
            "2365\n",
            "2366\n",
            "2367\n",
            "2368\n",
            "2369\n",
            "2370\n",
            "2371\n",
            "2372\n",
            "2373\n",
            "2374\n",
            "2375\n",
            "2376\n",
            "2377\n",
            "2378\n",
            "2379\n",
            "2380\n",
            "2381\n",
            "2382\n",
            "2383\n",
            "2384\n",
            "2385\n",
            "2386\n",
            "2387\n",
            "2388\n",
            "2389\n",
            "2390\n",
            "2391\n",
            "2392\n",
            "2393\n",
            "2394\n",
            "2395\n",
            "2396\n",
            "2397\n",
            "2398\n",
            "2399\n",
            "2400\n",
            "step: 2400, train loss: 3.151, val loss: 3.262\n",
            "2401\n",
            "2402\n",
            "2403\n",
            "2404\n",
            "2405\n",
            "2406\n",
            "2407\n",
            "2408\n",
            "2409\n",
            "2410\n",
            "2411\n",
            "2412\n",
            "2413\n",
            "2414\n",
            "2415\n",
            "2416\n",
            "2417\n",
            "2418\n",
            "2419\n",
            "2420\n",
            "2421\n",
            "2422\n",
            "2423\n",
            "2424\n",
            "2425\n",
            "2426\n",
            "2427\n",
            "2428\n",
            "2429\n",
            "2430\n",
            "2431\n",
            "2432\n",
            "2433\n",
            "2434\n",
            "2435\n",
            "2436\n",
            "2437\n",
            "2438\n",
            "2439\n",
            "2440\n",
            "2441\n",
            "2442\n",
            "2443\n",
            "2444\n",
            "2445\n",
            "2446\n",
            "2447\n",
            "2448\n",
            "2449\n",
            "2450\n",
            "2451\n",
            "2452\n",
            "2453\n",
            "2454\n",
            "2455\n",
            "2456\n",
            "2457\n",
            "2458\n",
            "2459\n",
            "2460\n",
            "2461\n",
            "2462\n",
            "2463\n",
            "2464\n",
            "2465\n",
            "2466\n",
            "2467\n",
            "2468\n",
            "2469\n",
            "2470\n",
            "2471\n",
            "2472\n",
            "2473\n",
            "2474\n",
            "2475\n",
            "2476\n",
            "2477\n",
            "2478\n",
            "2479\n",
            "2480\n",
            "2481\n",
            "2482\n",
            "2483\n",
            "2484\n",
            "2485\n",
            "2486\n",
            "2487\n",
            "2488\n",
            "2489\n",
            "2490\n",
            "2491\n",
            "2492\n",
            "2493\n",
            "2494\n",
            "2495\n",
            "2496\n",
            "2497\n",
            "2498\n",
            "2499\n",
            "2500\n",
            "step: 2500, train loss: 3.140, val loss: 3.248\n",
            "2501\n",
            "2502\n",
            "2503\n",
            "2504\n",
            "2505\n",
            "2506\n",
            "2507\n",
            "2508\n",
            "2509\n",
            "2510\n",
            "2511\n",
            "2512\n",
            "2513\n",
            "2514\n",
            "2515\n",
            "2516\n",
            "2517\n",
            "2518\n",
            "2519\n",
            "2520\n",
            "2521\n",
            "2522\n",
            "2523\n",
            "2524\n",
            "2525\n",
            "2526\n",
            "2527\n",
            "2528\n",
            "2529\n",
            "2530\n",
            "2531\n",
            "2532\n",
            "2533\n",
            "2534\n",
            "2535\n",
            "2536\n",
            "2537\n",
            "2538\n",
            "2539\n",
            "2540\n",
            "2541\n",
            "2542\n",
            "2543\n",
            "2544\n",
            "2545\n",
            "2546\n",
            "2547\n",
            "2548\n",
            "2549\n",
            "2550\n",
            "2551\n",
            "2552\n",
            "2553\n",
            "2554\n",
            "2555\n",
            "2556\n",
            "2557\n",
            "2558\n",
            "2559\n",
            "2560\n",
            "2561\n",
            "2562\n",
            "2563\n",
            "2564\n",
            "2565\n",
            "2566\n",
            "2567\n",
            "2568\n",
            "2569\n",
            "2570\n",
            "2571\n",
            "2572\n",
            "2573\n",
            "2574\n",
            "2575\n",
            "2576\n",
            "2577\n",
            "2578\n",
            "2579\n",
            "2580\n",
            "2581\n",
            "2582\n",
            "2583\n",
            "2584\n",
            "2585\n",
            "2586\n",
            "2587\n",
            "2588\n",
            "2589\n",
            "2590\n",
            "2591\n",
            "2592\n",
            "2593\n",
            "2594\n",
            "2595\n",
            "2596\n",
            "2597\n",
            "2598\n",
            "2599\n",
            "2600\n",
            "step: 2600, train loss: 3.144, val loss: 3.254\n",
            "2601\n",
            "2602\n",
            "2603\n",
            "2604\n",
            "2605\n",
            "2606\n",
            "2607\n",
            "2608\n",
            "2609\n",
            "2610\n",
            "2611\n",
            "2612\n",
            "2613\n",
            "2614\n",
            "2615\n",
            "2616\n",
            "2617\n",
            "2618\n",
            "2619\n",
            "2620\n",
            "2621\n",
            "2622\n",
            "2623\n",
            "2624\n",
            "2625\n",
            "2626\n",
            "2627\n",
            "2628\n",
            "2629\n",
            "2630\n",
            "2631\n",
            "2632\n",
            "2633\n",
            "2634\n",
            "2635\n",
            "2636\n",
            "2637\n",
            "2638\n",
            "2639\n",
            "2640\n",
            "2641\n",
            "2642\n",
            "2643\n",
            "2644\n",
            "2645\n",
            "2646\n",
            "2647\n",
            "2648\n",
            "2649\n",
            "2650\n",
            "2651\n",
            "2652\n",
            "2653\n",
            "2654\n",
            "2655\n",
            "2656\n",
            "2657\n",
            "2658\n",
            "2659\n",
            "2660\n",
            "2661\n",
            "2662\n",
            "2663\n",
            "2664\n",
            "2665\n",
            "2666\n",
            "2667\n",
            "2668\n",
            "2669\n",
            "2670\n",
            "2671\n",
            "2672\n",
            "2673\n",
            "2674\n",
            "2675\n",
            "2676\n",
            "2677\n",
            "2678\n",
            "2679\n",
            "2680\n",
            "2681\n",
            "2682\n",
            "2683\n",
            "2684\n",
            "2685\n",
            "2686\n",
            "2687\n",
            "2688\n",
            "2689\n",
            "2690\n",
            "2691\n",
            "2692\n",
            "2693\n",
            "2694\n",
            "2695\n",
            "2696\n",
            "2697\n",
            "2698\n",
            "2699\n",
            "2700\n",
            "step: 2700, train loss: 3.141, val loss: 3.260\n",
            "2701\n",
            "2702\n",
            "2703\n",
            "2704\n",
            "2705\n",
            "2706\n",
            "2707\n",
            "2708\n",
            "2709\n",
            "2710\n",
            "2711\n",
            "2712\n",
            "2713\n",
            "2714\n",
            "2715\n",
            "2716\n",
            "2717\n",
            "2718\n",
            "2719\n",
            "2720\n",
            "2721\n",
            "2722\n",
            "2723\n",
            "2724\n",
            "2725\n",
            "2726\n",
            "2727\n",
            "2728\n",
            "2729\n",
            "2730\n",
            "2731\n",
            "2732\n",
            "2733\n",
            "2734\n",
            "2735\n",
            "2736\n",
            "2737\n",
            "2738\n",
            "2739\n",
            "2740\n",
            "2741\n",
            "2742\n",
            "2743\n",
            "2744\n",
            "2745\n",
            "2746\n",
            "2747\n",
            "2748\n",
            "2749\n",
            "2750\n",
            "2751\n",
            "2752\n",
            "2753\n",
            "2754\n",
            "2755\n",
            "2756\n",
            "2757\n",
            "2758\n",
            "2759\n",
            "2760\n",
            "2761\n",
            "2762\n",
            "2763\n",
            "2764\n",
            "2765\n",
            "2766\n",
            "2767\n",
            "2768\n",
            "2769\n",
            "2770\n",
            "2771\n",
            "2772\n",
            "2773\n",
            "2774\n",
            "2775\n",
            "2776\n",
            "2777\n",
            "2778\n",
            "2779\n",
            "2780\n",
            "2781\n",
            "2782\n",
            "2783\n",
            "2784\n",
            "2785\n",
            "2786\n",
            "2787\n",
            "2788\n",
            "2789\n",
            "2790\n",
            "2791\n",
            "2792\n",
            "2793\n",
            "2794\n",
            "2795\n",
            "2796\n",
            "2797\n",
            "2798\n",
            "2799\n",
            "2800\n",
            "step: 2800, train loss: 3.141, val loss: 3.252\n",
            "2801\n",
            "2802\n",
            "2803\n",
            "2804\n",
            "2805\n",
            "2806\n",
            "2807\n",
            "2808\n",
            "2809\n",
            "2810\n",
            "2811\n",
            "2812\n",
            "2813\n",
            "2814\n",
            "2815\n",
            "2816\n",
            "2817\n",
            "2818\n",
            "2819\n",
            "2820\n",
            "2821\n",
            "2822\n",
            "2823\n",
            "2824\n",
            "2825\n",
            "2826\n",
            "2827\n",
            "2828\n",
            "2829\n",
            "2830\n",
            "2831\n",
            "2832\n",
            "2833\n",
            "2834\n",
            "2835\n",
            "2836\n",
            "2837\n",
            "2838\n",
            "2839\n",
            "2840\n",
            "2841\n",
            "2842\n",
            "2843\n",
            "2844\n",
            "2845\n",
            "2846\n",
            "2847\n",
            "2848\n",
            "2849\n",
            "2850\n",
            "2851\n",
            "2852\n",
            "2853\n",
            "2854\n",
            "2855\n",
            "2856\n",
            "2857\n",
            "2858\n",
            "2859\n",
            "2860\n",
            "2861\n",
            "2862\n",
            "2863\n",
            "2864\n",
            "2865\n",
            "2866\n",
            "2867\n",
            "2868\n",
            "2869\n",
            "2870\n",
            "2871\n",
            "2872\n",
            "2873\n",
            "2874\n",
            "2875\n",
            "2876\n",
            "2877\n",
            "2878\n",
            "2879\n",
            "2880\n",
            "2881\n",
            "2882\n",
            "2883\n",
            "2884\n",
            "2885\n",
            "2886\n",
            "2887\n",
            "2888\n",
            "2889\n",
            "2890\n",
            "2891\n",
            "2892\n",
            "2893\n",
            "2894\n",
            "2895\n",
            "2896\n",
            "2897\n",
            "2898\n",
            "2899\n",
            "2900\n",
            "step: 2900, train loss: 3.147, val loss: 3.256\n",
            "2901\n",
            "2902\n",
            "2903\n",
            "2904\n",
            "2905\n",
            "2906\n",
            "2907\n",
            "2908\n",
            "2909\n",
            "2910\n",
            "2911\n",
            "2912\n",
            "2913\n",
            "2914\n",
            "2915\n",
            "2916\n",
            "2917\n",
            "2918\n",
            "2919\n",
            "2920\n",
            "2921\n",
            "2922\n",
            "2923\n",
            "2924\n",
            "2925\n",
            "2926\n",
            "2927\n",
            "2928\n",
            "2929\n",
            "2930\n",
            "2931\n",
            "2932\n",
            "2933\n",
            "2934\n",
            "2935\n",
            "2936\n",
            "2937\n",
            "2938\n",
            "2939\n",
            "2940\n",
            "2941\n",
            "2942\n",
            "2943\n",
            "2944\n",
            "2945\n",
            "2946\n",
            "2947\n",
            "2948\n",
            "2949\n",
            "2950\n",
            "2951\n",
            "2952\n",
            "2953\n",
            "2954\n",
            "2955\n",
            "2956\n",
            "2957\n",
            "2958\n",
            "2959\n",
            "2960\n",
            "2961\n",
            "2962\n",
            "2963\n",
            "2964\n",
            "2965\n",
            "2966\n",
            "2967\n",
            "2968\n",
            "2969\n",
            "2970\n",
            "2971\n",
            "2972\n",
            "2973\n",
            "2974\n",
            "2975\n",
            "2976\n",
            "2977\n",
            "2978\n",
            "2979\n",
            "2980\n",
            "2981\n",
            "2982\n",
            "2983\n",
            "2984\n",
            "2985\n",
            "2986\n",
            "2987\n",
            "2988\n",
            "2989\n",
            "2990\n",
            "2991\n",
            "2992\n",
            "2993\n",
            "2994\n",
            "2995\n",
            "2996\n",
            "2997\n",
            "2998\n",
            "2999\n",
            "3.1924498081207275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device = device)\n",
        "generated_chars - decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)\n"
      ],
      "metadata": {
        "id": "KT14yUK_DcvN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}